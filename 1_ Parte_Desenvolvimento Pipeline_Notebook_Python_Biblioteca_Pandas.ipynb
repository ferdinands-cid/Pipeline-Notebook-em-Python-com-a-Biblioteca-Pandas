{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRONZE: Contém o arquivo disponibilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo duplicado com sucesso para: C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Para acessar o arquivo Vendas.csv na pasta BRONZE, precisei duplicar o arquivo original.\n",
    "# A duplicação do arquivo se fez necessária para garantir que possamos acessá-lo de acordo com a estrutura original e as permissões de segurança, mantendo a integridade dos dados baixados.\n",
    "\n",
    "# Para fazer essa operação, usei a biblioteca shutil para copiar o arquivo e o DataFrame Pandas para ler o arquivo copiado.\n",
    "import shutil\n",
    "\n",
    "# Caminho do arquivo de origem\n",
    "caminho_origem = r'C:\\Users\\ferna\\Downloads\\194883\\Vendas.csv'\n",
    "\n",
    "# Caminho da pasta de destino + nome do arquivo de origem\n",
    "caminho_destino = r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv'\n",
    "\n",
    "# Duplicar (copiar) o arquivo para o destino\n",
    "# Utilizei o shutil.copy() em vez de shutil.move() para justamente manter a integridade dos dados baixados.\n",
    "shutil.copy(caminho_origem, caminho_destino)\n",
    "\n",
    "# Lendo o arquivo copiado em um DataFrame Pandas\n",
    "df = pd.read_csv(caminho_destino)\n",
    "\n",
    "# Exibindo o DataFrame\n",
    "print(\"Arquivo duplicado com sucesso para:\", caminho_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        IDProduto                     Data  IDCliente  IDCampanha  Unidades  \\\n",
      "0             449  2012-07-26 00:00:00.000     247546          22         1   \n",
      "1             449  2013-10-31 00:00:00.000     124593          22         1   \n",
      "2             449  2013-11-14 00:00:00.000     163517          22         1   \n",
      "3             449  2013-01-17 00:00:00.000       8875          18         1   \n",
      "4             449  2014-09-13 00:00:00.000       8894          21         1   \n",
      "...           ...                      ...        ...         ...       ...   \n",
      "675363        407  2012-05-20 00:00:00.000      32464           3         1   \n",
      "675364        407  2011-03-02 00:00:00.000      32441           5         1   \n",
      "675365        407  2012-06-02 00:00:00.000     101533           1         1   \n",
      "675366        407  2011-04-10 00:00:00.000     147192           4         1   \n",
      "675367        407  2012-03-01 00:00:00.000     237766           3         1   \n",
      "\n",
      "              Produto  Categoria    Segmento  IDFabricante Fabricante  \\\n",
      "0       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
      "1       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
      "2       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
      "3       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
      "4       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
      "...               ...        ...         ...           ...        ...   \n",
      "675363  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
      "675364  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
      "675365  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
      "675366  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
      "675367  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
      "\n",
      "        CustoUnitario  PrecoUnitario  CodigoPostal  \\\n",
      "0           74.729917      102.36975         33194   \n",
      "1           74.729917      102.36975         33177   \n",
      "2           74.729917      102.36975         33172   \n",
      "3           74.729917      102.36975         33169   \n",
      "4           74.729917      102.36975         33169   \n",
      "...               ...            ...           ...   \n",
      "675363     116.887417      160.11975         32404   \n",
      "675364     116.887417      160.11975         32404   \n",
      "675365     116.887417      160.11975         32514   \n",
      "675366     116.887417      160.11975         32407   \n",
      "675367     116.887417      160.11975         32438   \n",
      "\n",
      "                                                EmailNome  \\\n",
      "0                    (Nerea.Barry@xyza.com): Barry, Nerea   \n",
      "1              (Elliott.Stuart@xyza.com): Stuart, Elliott   \n",
      "2              (Holmes.Swanson@xyza.com): Swanson, Holmes   \n",
      "3                    (Nyssa.Solis@xyza.com): Solis, Nyssa   \n",
      "4                      (Ivana.Wall@xyza.com): Wall, Ivana   \n",
      "...                                                   ...   \n",
      "675363  (Catherine.Martinez@xyza.com): Martinez, Cathe...   \n",
      "675364             (Dora.Emerson@xyza.com): Emerson, Dora   \n",
      "675365                 (Lynn.Jones@xyza.com): Jones, Lynn   \n",
      "675366         (Maite.Cantrell@xyza.com): Cantrell, Maite   \n",
      "675367           (Yolanda.Byers@xyza.com): Byers, Yolanda   \n",
      "\n",
      "                            Cidade Estado Regiao      Distrito Pais  \n",
      "0                   Miami, FL, USA     FL   East  District #10  USA  \n",
      "1                   Miami, FL, USA     FL   East  District #10  USA  \n",
      "2                   Miami, FL, USA     FL   East  District #10  USA  \n",
      "3                   Miami, FL, USA     FL   East  District #10  USA  \n",
      "4                   Miami, FL, USA     FL   East  District #10  USA  \n",
      "...                            ...    ...    ...           ...  ...  \n",
      "675363        Panama City, FL, USA     FL   East  District #09  USA  \n",
      "675364        Panama City, FL, USA     FL   East  District #09  USA  \n",
      "675365          Pensacola, FL, USA     FL   East  District #09  USA  \n",
      "675366  Panama City Beach, FL, USA     FL   East  District #09  USA  \n",
      "675367           Fountain, FL, USA     FL   East  District #09  USA  \n",
      "\n",
      "[675368 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "# Lendo o arquivo .CSV \n",
    "df_original = pd.read_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv')\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "print(df_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IDProduto                     Data  IDCliente  IDCampanha  Unidades  \\\n",
      "0        449  2012-07-26 00:00:00.000     247546          22         1   \n",
      "1        449  2013-10-31 00:00:00.000     124593          22         1   \n",
      "\n",
      "         Produto Categoria    Segmento  IDFabricante Fabricante  \\\n",
      "0  Maximus UM-54     Urban  Moderation             7  VanArsdel   \n",
      "1  Maximus UM-54     Urban  Moderation             7  VanArsdel   \n",
      "\n",
      "   CustoUnitario  PrecoUnitario  CodigoPostal  \\\n",
      "0      74.729917      102.36975         33194   \n",
      "1      74.729917      102.36975         33177   \n",
      "\n",
      "                                    EmailNome          Cidade Estado Regiao  \\\n",
      "0        (Nerea.Barry@xyza.com): Barry, Nerea  Miami, FL, USA     FL   East   \n",
      "1  (Elliott.Stuart@xyza.com): Stuart, Elliott  Miami, FL, USA     FL   East   \n",
      "\n",
      "       Distrito Pais  \n",
      "0  District #10  USA  \n",
      "1  District #10  USA  \n"
     ]
    }
   ],
   "source": [
    "# Lendo o arquivo .CSV com a codificação \"utf-8\" e usando \",\" como separador\n",
    "# Parâmetro (encoding=\"utf-8\"), para ler arquivo que contém caracteres especiais.\n",
    "# Parâmetro (sep= \",\"), separador utilizado para delimitar os valores dentro do arquivo CSV.\n",
    "df_original = pd.read_csv(r\"C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv\", encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "print(df_original.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SILVER: Transformações de Dados\n",
    "\n",
    "# Tratar a coluna nome, separando e-mail de nome\n",
    "# Tratar a coluna cidade, mantendo apenas o nome da cidade.\n",
    "# Eliminar as colunas IDCampanha, distrito e código posta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IDProduto                     Data  IDCliente  IDCampanha  Unidades  \\\n",
      "0        449  2012-07-26 00:00:00.000     247546          22         1   \n",
      "1        449  2013-10-31 00:00:00.000     124593          22         1   \n",
      "\n",
      "         Produto Categoria    Segmento  IDFabricante Fabricante  ...  \\\n",
      "0  Maximus UM-54     Urban  Moderation             7  VanArsdel  ...   \n",
      "1  Maximus UM-54     Urban  Moderation             7  VanArsdel  ...   \n",
      "\n",
      "   PrecoUnitario  CodigoPostal                                   EmailNome  \\\n",
      "0      102.36975         33194        (Nerea.Barry@xyza.com): Barry, Nerea   \n",
      "1      102.36975         33177  (Elliott.Stuart@xyza.com): Stuart, Elliott   \n",
      "\n",
      "           Cidade Estado Regiao      Distrito Pais                      Email  \\\n",
      "0  Miami, FL, USA     FL   East  District #10  USA     (Nerea.Barry@xyza.com)   \n",
      "1  Miami, FL, USA     FL   East  District #10  USA  (Elliott.Stuart@xyza.com)   \n",
      "\n",
      "             Nome  \n",
      "0     Barry Nerea  \n",
      "1  Stuart Elliott  \n",
      "\n",
      "[2 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# TRATAMENTO DA COLUNA NOME, SEPARANDO E-MAIL DE NOME:\n",
    "\n",
    "\n",
    "# Carregar o arquivo CSV\n",
    "df_original = pd.read_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv')\n",
    "\n",
    "# Usando o método str.split() da biblioteca Pandas, dividir o valor da coluna 'EmailNome' em duas colunas 'EmailNome' \n",
    "# com base no delimitador ':' para separar o e-mail do nome\n",
    "# A operação para clasificação da ordem das colunas 'Email' = [0] e 'Nome'= [1]\n",
    "# Usar .str.replace para substituir a vírgula por uma string vazia do nome\n",
    "# Usar .str.strip() para remover espaços em branco extras do nome\n",
    "\n",
    "# Separe o e-mail e o nome da coluna 'EmailNome' e atribua-os ao DataFrame original\n",
    "df_original['Email'] = df_original['EmailNome'].str.split(':', expand=True)[0]\n",
    "df_original['Nome'] = df_original['EmailNome'].str.split(':', expand=True)[1].str.replace(',', '').str.strip()\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "print(df_original.head(2))\n",
    "\n",
    "# Salvar o DataFrame modificado de volta no mesmo arquivo CSV\n",
    "df_original.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv', index=False) \n",
    "\n",
    "# Salvar o DataFrame modificado no formato Parquet\n",
    "df_original.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\SILVER\\Colunas_Nome_Transformada.parquet', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRATAMENTO DA COLUNA NOME, SEPARANDO E-MAIL DE NOME:\n",
    "\n",
    "# OBS: Só com esse metodo não salvada na Base de dados.\n",
    "\n",
    "\n",
    "# Defina os dados\n",
    "# dados = {'EmailNome': ['Nerea.Barry@xyza.com: Barry, Nerea','Elliott.Stuart@xyza.com: Stuart, Elliott','Holmes.Swanson@xyza.com: Swanson, Holmes']}\n",
    "\n",
    "# Crie o DataFrame df_original com os dados fornecidos\n",
    "# df_original  = pd.DataFrame(dados)\n",
    "\n",
    "\n",
    "# Usando o método str.split() da biblioteca Pandas, dividi o valor da coluna 'EmailNome' em duas colunas 'EmailNome' \n",
    "# com base no delimitador ':' para separar o e-mail do nome\n",
    "# A operação para clasificação da ordem das colunas 'Email' = [0] e 'Nome'= [1]\n",
    "# Usei .str.replace para substituir a vírgula por uma string vazia do nome\n",
    "# Usei .str.strip() para remover espaços em branco extras do nome\n",
    "\n",
    "# Separe o e-mail e o nome da coluna 'EmailNome' e atribua-os ao DataFrame original\n",
    "# df_original['Email'] = df_original['EmailNome'].str.split(':', expand=True)[0]\n",
    "# df_original['Nome'] = df_original['EmailNome'].str.split(':', expand=True)[1].str.replace(',', '').str.strip()\n",
    "\n",
    "# Exibindo o DataFrame resultante\n",
    "# print(df_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IDProduto                     Data  IDCliente  IDCampanha  Unidades  \\\n",
      "0        449  2012-07-26 00:00:00.000     247546          22         1   \n",
      "1        449  2013-10-31 00:00:00.000     124593          22         1   \n",
      "2        449  2013-11-14 00:00:00.000     163517          22         1   \n",
      "3        449  2013-01-17 00:00:00.000       8875          18         1   \n",
      "4        449  2014-09-13 00:00:00.000       8894          21         1   \n",
      "\n",
      "         Produto Categoria    Segmento  IDFabricante Fabricante  ...  \\\n",
      "0  Maximus UM-54     Urban  Moderation             7  VanArsdel  ...   \n",
      "1  Maximus UM-54     Urban  Moderation             7  VanArsdel  ...   \n",
      "2  Maximus UM-54     Urban  Moderation             7  VanArsdel  ...   \n",
      "3  Maximus UM-54     Urban  Moderation             7  VanArsdel  ...   \n",
      "4  Maximus UM-54     Urban  Moderation             7  VanArsdel  ...   \n",
      "\n",
      "   PrecoUnitario  CodigoPostal                                   EmailNome  \\\n",
      "0      102.36975         33194        (Nerea.Barry@xyza.com): Barry, Nerea   \n",
      "1      102.36975         33177  (Elliott.Stuart@xyza.com): Stuart, Elliott   \n",
      "2      102.36975         33172  (Holmes.Swanson@xyza.com): Swanson, Holmes   \n",
      "3      102.36975         33169        (Nyssa.Solis@xyza.com): Solis, Nyssa   \n",
      "4      102.36975         33169          (Ivana.Wall@xyza.com): Wall, Ivana   \n",
      "\n",
      "  Cidade Estado Regiao      Distrito Pais                      Email  \\\n",
      "0  Miami     FL   East  District #10  USA     (Nerea.Barry@xyza.com)   \n",
      "1  Miami     FL   East  District #10  USA  (Elliott.Stuart@xyza.com)   \n",
      "2  Miami     FL   East  District #10  USA  (Holmes.Swanson@xyza.com)   \n",
      "3  Miami     FL   East  District #10  USA     (Nyssa.Solis@xyza.com)   \n",
      "4  Miami     FL   East  District #10  USA      (Ivana.Wall@xyza.com)   \n",
      "\n",
      "             Nome  \n",
      "0     Barry Nerea  \n",
      "1  Stuart Elliott  \n",
      "2  Swanson Holmes  \n",
      "3     Solis Nyssa  \n",
      "4      Wall Ivana  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# TRATAR A COLUNA CIDADE, MANTENDO APENAS O NOME DA CIDADE\n",
    "\n",
    "\n",
    "# Carregar o arquivo CSV\n",
    "df_original = pd.read_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv')\n",
    "\n",
    "# Dividir a coluna 'Cidade' para manter apenas o nome da cidade\n",
    "# O método split() divide uma string em uma lista de substrings com base no separador especificado, neste caso, a vírgula.\n",
    "# Este processo é realizado dividindo cada valor da coluna 'Cidade' usando a vírgula como ponto de divisão.\n",
    "# Em seguida, selecionei apenas a primeira parte de cada valor, que corresponde ao nome da cidade, representada por str[0].\n",
    "df_original['Cidade'] = df_original['Cidade'].str.split(',').str[0]\n",
    "\n",
    "# Exiba o DataFrame resultante \n",
    "print(df_original.head())\n",
    "\n",
    "# Salvar o DataFrame modificado de volta no mesmo arquivo CSV\n",
    "df_original.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv', index=False) \n",
    "\n",
    "# Salvar o DataFrame modificado no formato Parquet\n",
    "df_original.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\SILVER\\Colunas_Cidade_Transformada.parquet', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELIMINAR AS COLUNAS IDCAMPANHA, DISTRITO E CÓDIGO POSTAL\n",
    "\n",
    "\n",
    "# Carregar o arquivo CSV\n",
    "df_original = pd.read_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv')\n",
    "\n",
    "# Definir as colunas a serem eliminadas\n",
    "columns_to_drop = ['IDCampanha', 'Distrito', 'CodigoPostal']\n",
    "\n",
    "# Verificar se as colunas já foram removidas anteriormente\n",
    "columns_already_dropped = [column for column in columns_to_drop if column not in df_original.columns]\n",
    "\n",
    "# Se não,\n",
    "if columns_already_dropped:\n",
    "    for column in columns_already_dropped:\n",
    "        print(f\"As colunas '{columns_to_drop}' foram removidas com sucesso!\")\n",
    "        print(df_original.head(2))  # Exibir as primeiras linhas do DataFrame original\n",
    "else:\n",
    "    # Eliminando as colunas IDCampanha, Distrito e CodigoPostal, se estiverem presentes\n",
    "    df_original = df_original.drop(columns_to_drop, axis=1)\n",
    "    # Removendo as colunas especificadas do DataFrame. \n",
    "    # axis=1 significa que estamos removendo colunas (em oposição a linhas)\n",
    "\n",
    "# Exiba o DataFrame resultante \n",
    "# print(df_original.head())\n",
    "\n",
    "# Salvar o DataFrame modificado de volta no mesmo arquivo CSV\n",
    "df_original.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv', index=False) \n",
    "\n",
    "# Salvar o DataFrame modificado no formato Parquet\n",
    "df_original.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\SILVER\\Colunas_Eliminadas_IDCampanha_Distrito_CodigoPostal.parquet', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOLD: Contém os arquivos prontos para uso em formato parquet\n",
    "\n",
    "# Adicionar as SK´s as tabelas dimensões - OK\n",
    "# Aplicar SCD1 nas dimensões;\n",
    "# Adicionar as SK´s tabela fato - \n",
    "# Particionar os arquivos Parquet da tabela fato em ano e mês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sk_Produto   NOME_PRODUTO\n",
      "0            0  Maximus UM-54\n",
      "1            1  Maximus UM-75\n",
      "2            2  Maximus UM-01\n",
      "3            3  Maximus UM-62\n",
      "4            4  Maximus UM-38\n",
      "..         ...            ...\n",
      "168        168  Maximus RS-01\n",
      "169        169  Maximus RP-01\n",
      "170        170  Maximus RP-02\n",
      "171        171  Maximus UM-05\n",
      "172        172  Maximus UM-07\n",
      "\n",
      "[173 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃO PRODUTO\n",
    "\n",
    "\n",
    "# Primeiro, criei um DataFrame chamado df_original_Produto a partir do DataFrame original, selecionando apenas a coluna \"Produto\".\n",
    "# Em seguida, apliquei método .drop_duplicates() do Pandase o pârametro (subset=), para remover as linhas duplicadas com base na coluna \"Produto\".\n",
    "# Essa operação será atribuida ao DataFrame criado\n",
    "\n",
    "# Supondo que df_original_Produto seja um DataFrame válido\n",
    "df_original_Produto = df_original.drop_duplicates(subset= 'Produto')\n",
    "\n",
    "# Criei um novo DataFrame chamado df_d_Produto para armazenar a dimensão Produto.\n",
    "# Este novo DataFrame terá duas colunas: \"sk_Produto\" para a chave única do produto e \"NOME_PRODUTO\" para o nome do produto.\n",
    "\n",
    "# Inicializamos df_d_Produto como um DataFrame vazio com as colunas necessárias\n",
    "df_d_Produto = pd.DataFrame(columns=['sk_Produto','NOME_PRODUTO'])\n",
    "\n",
    "# Em seguida, percorremos cada linha do DataFrame df_original_Produto para preencher o DataFrame df_d_Produto.\n",
    "for index, row in df_original_Produto.iterrows():\n",
    "    # Aqui 'index' representa o índice da linha atual e 'row' contém os dados da linha atual.\n",
    "    # Em cada iteração do loop, 'index' e 'row' representam os dados de uma linha específica do DataFrame.\n",
    "\n",
    "    # Aqui estamos criando uma nova linha de dados para adicionar ao DataFrame de dimensão Produto (df_d_Produto).\n",
    "    # Esta linha conterá a chave única (sk_Produto) e o nome do produto (NOME_PRODUTO).\n",
    "    new_row = pd.DataFrame({\n",
    "        \n",
    "        # A chave única do produto é atribuído automaticamente. Se já tivermos produtos registrados, o próximo ID será o maior ID atual + 1.\n",
    "        # Caso contrário, se o DataFrame estiver vazio, atribuímos o ID 0.\n",
    "        'sk_Produto': [df_d_Produto.index.max() + 1 if len(df_d_Produto) > 0 else 0],\n",
    "\n",
    "        #Aqui copiamos o nome do produto da linha atual do DataFrame original para a coluna \"NOME_PRODUTO\" desta nova linha.\n",
    "        'NOME_PRODUTO': [row['Produto']]\n",
    "    })\n",
    "\n",
    "    # Agora que temos uma nova linha de dados pronta, precisamos adicioná-la ao DataFrame de dimensão Produto (df_d_Produto).\n",
    "\n",
    "    # Usando a função pd.concat, concatenei o novo DataFrame (new_row) ao DataFrame df_d_Produto\n",
    "    # O parâmetro ignore_index=True garante que os índices sejam redefinidos após a concatenação, garantindo uma sequência contínua de índices.\n",
    "    df_d_Produto = pd.concat([df_d_Produto, new_row], ignore_index=True)\n",
    "\n",
    "# Exibe o novo DataFrame df_d_Produto\n",
    "# Este DataFrame agora contém chave única para cada Produto e seus respectivos nomes\n",
    "print(df_d_Produto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sk_Categoria NOME_CATEGORIA\n",
      "0            0          Urban\n",
      "1            1            Mix\n",
      "2            2          Youth\n",
      "3            3          Rural\n",
      "4            4      Accessory\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃO CATEGORIA\n",
    "\n",
    "\n",
    "# Primeiro, criei um DataFrame chamado df_original_Categoria a partir do DataFrame original, selecionando apenas a coluna \"Categoria\".\n",
    "# Em seguida, apliquei método .drop_duplicates() do Pandase o pârametro (subset=), para remover as linhas duplicadas com base na coluna \"Produto\".\n",
    "# Essa operação será atribuida ao DataFrame criado\n",
    "\n",
    "# Supondo que df_original_Produto seja um DataFrame válido\n",
    "df_original_Categoria = df_original.drop_duplicates(subset='Categoria')\n",
    "\n",
    "# Criei um novo DataFrame chamado df_d_Produto para armazenar a dimensão Produto.\n",
    "# Este novo DataFrame terá duas colunas: \"sk_Categoria\" para a chave única do produto e \"NOME_CATEGORIA\" para o nome do produto.\n",
    "\n",
    "# Inicializamos df_d_Categoria como um DataFrame vazio com as colunas necessárias\n",
    "df_d_Categoria = pd.DataFrame(columns=['sk_Categoria', 'NOME_CATEGORIA'])\n",
    "\n",
    "# Em seguida, percorremos cada linha do DataFrame df_original_Produto para preencher o DataFrame df_d_Produto.\n",
    "for index, row in df_original_Categoria.iterrows():\n",
    "    # Aqui 'index' representa o índice da linha atual e 'row' contém os dados da linha atual.\n",
    "    # Em cada iteração do loop, 'index' e 'row' representam os dados de uma linha específica do DataFrame.\n",
    "\n",
    "    # Aqui estamos criando uma nova linha de dados para adicionar ao DataFrame de dimensão Produto (df_d_Produto).\n",
    "    # Esta linha conterá a chave única (sk_Produto) e o nome do produto (NOME_PRODUTO).\n",
    "    new_row = pd.DataFrame({\n",
    "        \n",
    "        # A chave única do produto é atribuído automaticamente. Se já tivermos produtos registrados, o próximo ID será o maior ID atual + 1.\n",
    "        # Caso contrário, se o DataFrame estiver vazio, atribuímos o ID 0.\n",
    "        'sk_Categoria': [df_d_Categoria.index.max() + 1 if len(df_d_Categoria) > 0 else 0],\n",
    "\n",
    "        #Aqui copiamos o nome do produto da linha atual do DataFrame original para a coluna \"NOME_CATEGORIA\" desta nova linha.\n",
    "        'NOME_CATEGORIA': [row['Categoria']]\n",
    "    })\n",
    "\n",
    "    # Agora que temos uma nova linha de dados pronta, precisamos adicioná-la ao DataFrame de dimensão Produto (df_d_Categoria).\n",
    "\n",
    "    # Usando a função pd.concat, concatenei o novo DataFrame (new_row) ao DataFrame df_d_Produto\n",
    "    # O parâmetro ignore_index=True garante que os índices sejam redefinidos após a concatenação, garantindo uma sequência contínua de índices.\n",
    "    df_d_Categoria = pd.concat([df_d_Categoria, new_row], ignore_index=True)\n",
    "\n",
    "# Exibe o novo DataFrame df_d_Produto\n",
    "# Este DataFrame agora contém chave única para cada Produto e seus respectivos nomes\n",
    "print(df_d_Categoria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sk_Segmento NOME_SEGMENTO\n",
      "0           0    Moderation\n",
      "1           1   Convenience\n",
      "2           2       Extreme\n",
      "3           3       Regular\n",
      "4           4    All Season\n",
      "5           5  Productivity\n",
      "6           6         Youth\n",
      "7           7        Select\n",
      "8           8     Accessory\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃO SEGMENTO\n",
    "\n",
    "\n",
    "# Primeiro, criei um DataFrame chamado df_original_Segmento a partir do DataFrame original, selecionando apenas a coluna \"Segmento\".\n",
    "# Em seguida, apliquei método .drop_duplicates() do Pandase o pârametro (subset=), para remover as linhas duplicadas com base na coluna \"Segmento\".\n",
    "# Essa operação será atribuida ao DataFrame criado\n",
    "\n",
    "# Supondo que df_original_Segmento seja um DataFrame válido\n",
    "df_original_Segmento = df_original.drop_duplicates(subset='Segmento')\n",
    "\n",
    "# Criei um novo DataFrame chamado df_d_Segmento para armazenar a dimensão Segmento.\n",
    "# Este novo DataFrame terá duas colunas: \"sk_Segmento\" para a chave única do Segmento e \"NOME_SEGMENTO\" para o nome de Segmento.\n",
    "\n",
    "# Inicializamos df_d_Segmento como um DataFrame vazio com as colunas necessárias.\n",
    "df_d_Segmento = pd.DataFrame(columns=['sk_Segmento','NOME_SEGMENTO'])\n",
    "\n",
    "# Em seguida, percorremos cada linha do DataFrame df_original_Segmento para preencher o DataFrame df_d_Segmento\n",
    "for index, row in df_original_Segmento.iterrows():\n",
    "    # Aqui 'index' representa o índice da linha atual e 'row' contém os dados da linha atual.\n",
    "    # Em cada iteração do loop, 'index' e 'row' representam os dados de uma linha específica do DataFrame.\n",
    "\n",
    "    # Aqui estamos criando uma nova linha de dados para adicionar ao DataFrame de dimensão Segmento (df_d_Segmento).\n",
    "    # Esta linha conterá a chave única (sk_Segmento) e o nome do Segmento (NOME_SEGMENTO).\n",
    "    new_row = pd.DataFrame({\n",
    "        \n",
    "        # A chave única do Segmento é atribuído automaticamente. Se já tivermos Segmento registradas, o próximo ID será o maior ID atual + 1.\n",
    "        # Caso contrário, se o DataFrame estiver vazio, atribuímos o ID 0.\n",
    "        'sk_Segmento': [df_d_Segmento.index.max() + 1 if len(df_d_Segmento) > 0 else 0],\n",
    "\n",
    "        #Aqui copiamos o nome do Segmento da linha atual do DataFrame original para a coluna \"NOME_SEGMENTO\" desta nova linha.\n",
    "        'NOME_SEGMENTO': [row['Segmento']]\n",
    "    })\n",
    "\n",
    "    # Agora que temos uma nova linha de dados pronta, precisamos adicioná-la ao DataFrame de dimensão Segmento (df_d_Segmento).\n",
    "\n",
    "    # Usando a função pd.concat, concatenei o novo DataFrame (new_row) ao DataFrame df_d_Segmento\n",
    "    # O parâmetro ignore_index=True garante que os índices sejam redefinidos após a concatenação, garantindo uma sequência contínua de índices.\n",
    "    df_d_Segmento = pd.concat([df_d_Segmento, new_row], ignore_index=True)\n",
    "\n",
    "# Exibe o novo DataFrame df_d_Segmento\n",
    "# Este DataFrame agora contém chave única para cada Segmento e seus respectivos nomes\n",
    "print(df_d_Segmento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sk_Fabricante NOME_FABRICANTE\n",
      "0             0       VanArsdel\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃO FABRICANTE\n",
    "\n",
    "\n",
    "# Primeiro, criei um DataFrame chamado df_original_Fabricante a partir do DataFrame original, selecionando apenas a coluna \"Fabricante\".\n",
    "# Em seguida, apliquei método .drop_duplicates() do Pandase o pârametro (subset=), para remover as linhas duplicadas com base na coluna \"Fabricante\".\n",
    "# Essa operação será atribuida ao DataFrame criado\n",
    "\n",
    "# Supondo que df_original_Fabricante seja um DataFrame válido\n",
    "df_original_Fabricante = df_original.drop_duplicates(subset='Fabricante')\n",
    "\n",
    "# Criei um novo DataFrame chamado df_d_Fabricante para armazenar a dimensão Fabricante.\n",
    "# Este novo DataFrame terá duas colunas: \"sk_Fabricante\" para a chave única do Fabricante e \"NOME_FABRICANTE\" para o nome de Fabricante.\n",
    "\n",
    "# Inicializamos df_d_Fabricante como um DataFrame vazio com as colunas necessárias.\n",
    "df_d_Fabricante = pd.DataFrame(columns=['sk_Fabricante','NOME_FABRICANTE'])\n",
    "\n",
    "# Em seguida, percorremos cada linha do DataFrame df_original_Fabricante para preencher o DataFrame df_d_Fabricante\n",
    "for index, row in df_original_Fabricante.iterrows():\n",
    "    # Aqui 'index' representa o índice da linha atual e 'row' contém os dados da linha atual.\n",
    "    # Em cada iteração do loop, 'index' e 'row' representam os dados de uma linha específica do DataFrame.\n",
    "\n",
    "    # Aqui estamos criando uma nova linha de dados para adicionar ao DataFrame de dimensão Fabricante (df_d_Fabricante).\n",
    "    # Esta linha conterá a chave única (sk_Fabricante) e o nome do Fabricante (NOME_FABRICANTE).\n",
    "    new_row = pd.DataFrame({\n",
    "        \n",
    "        # A chave única do Fabricante é atribuído automaticamente. Se já tivermos Fabricante registradas, o próximo ID será o maior ID atual + 1.\n",
    "        # Caso contrário, se o DataFrame estiver vazio, atribuímos o ID 0.\n",
    "        'sk_Fabricante': [df_d_Fabricante.index.max() + 1 if len(df_d_Fabricante) > 0 else 0],\n",
    "\n",
    "        #Aqui copiamos o nome do Fabricante da linha atual do DataFrame original para a coluna \"NOME_FABRICANTE\" desta nova linha.\n",
    "        'NOME_FABRICANTE': [row['Fabricante']]\n",
    "    })\n",
    "\n",
    "    # Agora que temos uma nova linha de dados pronta, precisamos adicioná-la ao DataFrame de dimensão Fabricante (df_d_Fabricante).\n",
    "\n",
    "    # Usando a função pd.concat, concatenei o novo DataFrame (new_row) ao DataFrame df_d_Fabricante\n",
    "    # O parâmetro ignore_index=True garante que os índices sejam redefinidos após a concatenação, garantindo uma sequência contínua de índices.\n",
    "    df_d_Fabricante = pd.concat([df_d_Fabricante, new_row], ignore_index=True)\n",
    "\n",
    "# Exibe o novo DataFrame df_d_Fabricante\n",
    "# Este DataFrame agora contém chave única para cada Fabricante e seus respectivos nomes\n",
    "print(df_d_Fabricante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sk_Nome          NOME_NOME\n",
      "0             1        Barry Nerea\n",
      "1             2     Stuart Elliott\n",
      "2             3     Swanson Holmes\n",
      "3             4        Solis Nyssa\n",
      "4             5         Wall Ivana\n",
      "...         ...                ...\n",
      "675127   250817    Guthrie Nichole\n",
      "675269   250818     Sloan Kirestin\n",
      "675308   250819        Floyd Venus\n",
      "675322   250820  Williams Patience\n",
      "675343   250821  Velazquez Aladdin\n",
      "\n",
      "[250821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃO NOME\n",
    "\n",
    "\n",
    "# Supondo que df_original_Nome seja um DataFrame válido\n",
    "# Remove as linhas duplicadas com base na coluna 'Nome' e armazena em df_original_Nome\n",
    "df_original_Nome = df_original.drop_duplicates(subset='Nome')\n",
    "\n",
    "# Crie um dicionário para mapear os nomes únicos de Nome para suas chaves únicas\n",
    "nome_to_sk = {}\n",
    "\n",
    "# Escolhi o método enumerate neste caso específico para melhorar o desempenho do código\n",
    "# Atribui chaves únicas para os nomes únicos\n",
    "for sk, nome in enumerate(df_original_Nome['Nome'].unique(), start=1):\n",
    "    nome_to_sk[nome] = sk\n",
    "\n",
    "# Cria o DataFrame de dimensão Nome\n",
    "df_d_Nome = pd.DataFrame({\n",
    "    'sk_Nome': [nome_to_sk[nome] for nome in df_original_Nome['Nome']],\n",
    "    'NOME_NOME': df_original_Nome['Nome']\n",
    "})\n",
    "\n",
    "# Exibe o novo DataFrame df_d_Nome\n",
    "# Este DataFrame agora contém chave única para cada Nome e seus respectivos nomes\n",
    "print(df_d_Nome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sk_Cidade       NOME_CIDADE\n",
      "0             0             Miami\n",
      "1             1   Fort Lauderdale\n",
      "2             2  Port Saint Lucie\n",
      "3             3        Fort Myers\n",
      "4             4             Tampa\n",
      "...         ...               ...\n",
      "14512     14512         Sassafras\n",
      "14513     14513           Mayking\n",
      "14514     14514           Gunlock\n",
      "14515     14515        Clairfield\n",
      "14516     14516      Apalachicola\n",
      "\n",
      "[14517 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃO CIDADE\n",
    "\n",
    "\n",
    "# Primeiro, criei um DataFrame chamado df_original_Cidade a partir do DataFrame original, selecionando apenas a coluna \"Cidade\".\n",
    "# Em seguida, apliquei método .drop_duplicates() do Pandase o pârametro (subset=), para remover as linhas duplicadas com base na coluna \"Cidade\".\n",
    "# Essa operação será atribuida ao DataFrame criado\n",
    "\n",
    "# Supondo que df_original_Cidade seja um DataFrame válido\n",
    "df_original_Cidade = df_original.drop_duplicates(subset='Cidade')\n",
    "\n",
    "# Criei um novo DataFrame chamado df_d_Cidade para armazenar a dimensão Cidade.\n",
    "# Este novo DataFrame terá duas colunas: \"sk_Cidade\" para a chave única da Cidade e \"NOME_CIDADE\" para o nome da Cidade.\n",
    "\n",
    "# Inicializamos df_d_Cidade como um DataFrame vazio com as colunas necessárias.\n",
    "df_d_Cidade = pd.DataFrame(columns=['sk_Cidade', 'NOME_CIDADE'])\n",
    "\n",
    "# Em seguida, percorremos cada linha do DataFrame df_original_Cidade para preencher o DataFrame df_d_Cidade\n",
    "for index, row in df_original_Cidade.iterrows():\n",
    "    # Aqui 'index' representa o índice da linha atual e 'row' contém os dados da linha atual.\n",
    "    # Em cada iteração do loop, 'index' e 'row' representam os dados de uma linha específica do DataFrame.\n",
    "\n",
    "    # Aqui estamos criando uma nova linha de dados para adicionar ao DataFrame de dimensão Cidade (df_d_Cidade.\n",
    "    # Esta linha conterá a chave única (sk_Cidade) e o nome da Cidade (NOME_CIDADE).\n",
    "    new_row = pd.DataFrame({\n",
    "        \n",
    "        # A chave única da Cidade é atribuída automaticamente. Se já tivermos Cidade registradas, o próximo ID será o maior ID atual + 1.\n",
    "        # Caso contrário, se o DataFrame estiver vazio, atribuímos o ID 0.\n",
    "        'sk_Cidade': [df_d_Cidade.index.max() + 1 if len(df_d_Cidade) > 0 else 0],\n",
    "\n",
    "        #Aqui copiamos o nome da Cidade da linha atual do DataFrame original para a coluna \"NOME_CIDADE\" desta nova linha.\n",
    "        'NOME_CIDADE': [row['Cidade']]\n",
    "    })\n",
    "\n",
    "    # Agora que temos uma nova linha de dados pronta, precisamos adicioná-la ao DataFrame de dimensão Nome (df_d_Cidade).\n",
    "\n",
    "    # Usando a função pd.concat, concatenei o novo DataFrame (new_row) ao DataFrame df_d_Cidade\n",
    "    # O parâmetro ignore_index=True garante que os índices sejam redefinidos após a concatenação, garantindo uma sequência contínua de índices.\n",
    "    df_d_Cidade = pd.concat([df_d_Cidade, new_row], ignore_index=True)\n",
    "\n",
    "# Exibe o novo DataFrame df_d_Cidade\n",
    "# Este DataFrame agora contém chave única para cada Cidade e seus respectivos nomes\n",
    "print(df_d_Cidade)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sk_Estado NOME_ESTADO\n",
      "0          0          FL\n",
      "1          1          OH\n",
      "2          2          PA\n",
      "3          3          NY\n",
      "4          4          MI\n",
      "5          5          NC\n",
      "6          6          IN\n",
      "7          7          GA\n",
      "8          8          NJ\n",
      "9          9          SC\n",
      "10        10          KY\n",
      "11        11          WV\n",
      "12        12          AL\n",
      "13        13          IL\n",
      "14        14          VA\n",
      "15        15          CT\n",
      "16        16          MD\n",
      "17        17          MA\n",
      "18        18          VT\n",
      "19        19          DE\n",
      "20        20          DC\n",
      "21        21          RI\n",
      "22        22          NH\n",
      "23        23          MS\n",
      "24        24          TN\n",
      "25        25          LA\n",
      "26        26          AR\n",
      "27        27          NM\n",
      "28        28          MO\n",
      "29        29          KS\n",
      "30        30          OK\n",
      "31        31          CO\n",
      "32        32          WY\n",
      "33        33          NE\n",
      "34        34          UT\n",
      "35        35          MT\n",
      "36        36          IA\n",
      "37        37          SD\n",
      "38        38          ND\n",
      "39        39          MN\n",
      "40        40          WI\n",
      "41        41          TX\n",
      "42        42          CA\n",
      "43        43          NV\n",
      "44        44          OR\n",
      "45        45          AK\n",
      "46        46          WA\n",
      "47        47          ID\n",
      "48        48          AZ\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃO ESTADO\n",
    "\n",
    "\n",
    "# Primeiro, criei um DataFrame chamado df_original_Estado a partir do DataFrame original, selecionando apenas a coluna \"Estado\".\n",
    "# Em seguida, apliquei método .drop_duplicates() do Pandase o pârametro (subset=), para remover as linhas duplicadas com base na coluna \"Estado\".\n",
    "# Essa operação será atribuida ao DataFrame criado\n",
    "\n",
    "# Supondo que df_original_Estado seja um DataFrame válido\n",
    "df_original_Estado = df_original.drop_duplicates(subset='Estado')\n",
    "\n",
    "# Criei um novo DataFrame chamado df_d_Estado para armazenar a dimensão Estado.\n",
    "# Este novo DataFrame terá duas colunas: \"sk_Estado\" para a chave única do Estado e \"NOME_ESTADO\" para o nome da Estado.\n",
    "\n",
    "# Inicializamos df_d_Estado como um DataFrame vazio com as colunas necessárias.\n",
    "df_d_Estado = pd.DataFrame(columns=['sk_Estado', 'NOME_ESTADO'])\n",
    "\n",
    "# Em seguida, percorremos cada linha do DataFrame df_original_Estado para preencher o DataFrame df_d_Estado\n",
    "for index, row in df_original_Estado.iterrows():\n",
    "    # Aqui 'index' representa o índice da linha atual e 'row' contém os dados da linha atual.\n",
    "    # Em cada iteração do loop, 'index' e 'row' representam os dados de uma linha específica do DataFrame.\n",
    "\n",
    "    # Aqui estamos criando uma nova linha de dados para adicionar ao DataFrame de dimensão Estado (df_d_Estado.\n",
    "    # Esta linha conterá a chave única (sk_Estado) e o nome do Estado (NOME_ESTADO).\n",
    "    new_row = pd.DataFrame({\n",
    "        \n",
    "        # A chave única do Estado é atribuída automaticamente. Se já tivermos Estado registrados, o próximo ID será o maior ID atual + 1.\n",
    "        # Caso contrário, se o DataFrame estiver vazio, atribuímos o ID 0.\n",
    "        'sk_Estado': [df_d_Estado.index.max() + 1 if len(df_d_Estado) > 0 else 0],\n",
    "\n",
    "        #Aqui copiamos o nome do Estado da linha atual do DataFrame original para a coluna \"NOME_ESTADO\" desta nova linha.\n",
    "        'NOME_ESTADO': [row['Estado']]\n",
    "    })\n",
    "\n",
    "    # Agora que temos uma nova linha de dados pronta, precisamos adicioná-la ao DataFrame de dimensão Nome (df_d_Estado).\n",
    "\n",
    "    # Usando a função pd.concat, concatenei o novo DataFrame (new_row) ao DataFrame df_d_Estado\n",
    "    # O parâmetro ignore_index=True garante que os índices sejam redefinidos após a concatenação, garantindo uma sequência contínua de índices.\n",
    "    df_d_Estado = pd.concat([df_d_Estado, new_row], ignore_index=True)\n",
    "\n",
    "# Exibe o novo DataFrame df_d_Estado\n",
    "# Este DataFrame agora contém chave única para cada Estado e seus respectivos nomes\n",
    "print(df_d_Estado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sk_Regiao NOME_REGIAO\n",
      "0         0        East\n",
      "1         1     Central\n",
      "2         2        West\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃO REGIÃO\n",
    "\n",
    "\n",
    "# Primeiro, criei um DataFrame chamado df_original_Regiao a partir do DataFrame original, selecionando apenas a coluna \"Regiao\".\n",
    "# Em seguida, apliquei método .drop_duplicates() do Pandase o pârametro (subset=), para remover as linhas duplicadas com base na coluna \"Regiao\".\n",
    "# Essa operação será atribuida ao DataFrame criado\n",
    "\n",
    "# Supondo que df_original_Regiao seja um DataFrame válido\n",
    "df_original_Regiao = df_original.drop_duplicates(subset='Regiao')\n",
    "\n",
    "# Criei um novo DataFrame chamado df_d_Regiao para armazenar a dimensão Regiao.\n",
    "# Este novo DataFrame terá duas colunas: \"sk_Regiao\" para a chave única da Regiao e \"NOME_REGIAO\" para o nome da Regiao.\n",
    "\n",
    "# Inicializamos df_d_Estado como um DataFrame vazio com as colunas necessárias.\n",
    "df_d_Regiao = pd.DataFrame(columns=['sk_Regiao','NOME_REGIAO'])\n",
    "\n",
    "# Em seguida, percorremos cada linha do DataFrame df_original_Regiao para preencher o DataFrame df_d_Regiao\n",
    "for index, row in df_original_Regiao.iterrows():\n",
    "    # Aqui 'index' representa o índice da linha atual e 'row' contém os dados da linha atual.\n",
    "    # Em cada iteração do loop, 'index' e 'row' representam os dados de uma linha específica do DataFrame.\n",
    "\n",
    "    # Aqui estamos criando uma nova linha de dados para adicionar ao DataFrame de dimensão Regiao (df_d_Regiao.\n",
    "    # Esta linha conterá a chave única (sk_Regiao) e o nome do Regiao (NOME_REGIAO).\n",
    "    new_row = pd.DataFrame({\n",
    "        \n",
    "        # A chave única do Regiao é atribuída automaticamente. Se já tivermos Regiao registradas, o próximo ID será o maior ID atual + 1.\n",
    "        # Caso contrário, se o DataFrame estiver vazio, atribuímos o ID 0.\n",
    "        'sk_Regiao': [df_d_Regiao.index.max() + 1 if len(df_d_Regiao) > 0 else 0],\n",
    "\n",
    "        #Aqui copiamos o nome do Regiao da linha atual do DataFrame original para a coluna \"NOME_REGIAO\" desta nova linha.\n",
    "        'NOME_REGIAO': [row['Regiao']]\n",
    "    })\n",
    "\n",
    "    # Agora que temos uma nova linha de dados pronta, precisamos adicioná-la ao DataFrame de dimensão Nome (df_d_Regiao).\n",
    "\n",
    "    # Usando a função pd.concat, concatenei o novo DataFrame (new_row) ao DataFrame df_d_Regiao\n",
    "    # O parâmetro ignore_index=True garante que os índices sejam redefinidos após a concatenação, garantindo uma sequência contínua de índices.\n",
    "    df_d_Regiao = pd.concat([df_d_Regiao, new_row], ignore_index=True)\n",
    "\n",
    "# Exibe o novo DataFrame df_d_Regiao\n",
    "# Este DataFrame agora contém chave única para cada Regiao e seus respectivos nomes\n",
    "print(df_d_Regiao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sk_Pais NOME_PAIS\n",
      "0       0       USA\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃO PAÍS\n",
    "\n",
    "\n",
    "# Primeiro, criei um DataFrame chamado df_original_Pais a partir do DataFrame original, selecionando apenas a coluna \"Pais\".\n",
    "# Em seguida, apliquei método .drop_duplicates() do Pandase o pârametro (subset=), para remover as linhas duplicadas com base na coluna \"Pais\".\n",
    "# Essa operação será atribuida ao DataFrame criado\n",
    "\n",
    "# Supondo que df_original_Pais seja um DataFrame válido\n",
    "df_original_Pais = df_original.drop_duplicates(subset='Pais')\n",
    "\n",
    "# Criei um novo DataFrame chamado df_d_Pais para armazenar a dimensão Pais.\n",
    "# Este novo DataFrame terá duas colunas: \"sk_Pais\" para a chave única de Pais e \"NOME_PAIS\" para o nome da Pais.\n",
    "\n",
    "# Inicializamos df_d_Pais como um DataFrame vazio com as colunas necessárias.\n",
    "df_d_Pais = pd.DataFrame(columns=['sk_Pais','NOME_PAIS'])\n",
    "\n",
    "# Em seguida, percorremos cada linha do DataFrame df_original_Pais para preencher o DataFrame df_d_Pais\n",
    "for index, row in df_original_Pais.iterrows(): \n",
    "    # Aqui 'index' representa o índice da linha atual e 'row' contém os dados da linha atual.\n",
    "    # Em cada iteração do loop, 'index' e 'row' representam os dados de uma linha específica do DataFrame.\n",
    "\n",
    "    # Aqui estamos criando uma nova linha de dados para adicionar ao DataFrame de dimensão Pais (df_d_Pais.\n",
    "    # Esta linha conterá a chave única (sk_Pais) e o nome do Pais (NOME_PAIS).\n",
    "    new_row = pd.DataFrame({\n",
    "        \n",
    "        # A chave única do Pais é atribuída automaticamente. Se já tivermos Pais registradas, o próximo ID será o maior ID atual + 1.\n",
    "        # Caso contrário, se o DataFrame estiver vazio, atribuímos o ID 0.\n",
    "        'sk_Pais': [df_d_Pais.index.max() + 1 if len(df_d_Pais) > 0 else 0],\n",
    "\n",
    "        #Aqui copiamos o nome do Pais da linha atual do DataFrame original para a coluna \"NOME_PAIS\" desta nova linha.\n",
    "        'NOME_PAIS': [row['Pais']]\n",
    "    })\n",
    "\n",
    "    # Agora que temos uma nova linha de dados pronta, precisamos adicioná-la ao DataFrame de dimensão Nome (df_d_Pais).\n",
    "\n",
    "    # Usando a função pd.concat, concatenei o novo DataFrame (new_row) ao DataFrame df_d_Pais\n",
    "    # O parâmetro ignore_index=True garante que os índices sejam redefinidos após a concatenação, garantindo uma sequência contínua de índices.\n",
    "    df_d_Pais = pd.concat([df_d_Pais, new_row], ignore_index=True)\n",
    "\n",
    "# Exibe o novo DataFrame df_d_Pais\n",
    "# Este DataFrame agora contém chave única para cada Pais e seus respectivos nomes\n",
    "print(df_d_Pais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame atualizado após aplicar SCD1:\n",
      "(     sk_Produto    NOME_PRODUTO\n",
      "0             0   Maximus UM-00\n",
      "1             1   Maximus UM-01\n",
      "2             2   Maximus UM-02\n",
      "3             3   Maximus UM-03\n",
      "4             4   Maximus UM-04\n",
      "..          ...             ...\n",
      "167         167  Maximus UM-167\n",
      "168         168  Maximus UM-168\n",
      "169         169  Maximus UM-169\n",
      "170         170  Maximus UM-170\n",
      "171         171  Maximus UM-171\n",
      "\n",
      "[172 rows x 2 columns],)\n",
      "(   sk_Categoria NOME_CATEGORIA\n",
      "0             0  Maximus UM-00\n",
      "1             1  Maximus UM-01\n",
      "2             2  Maximus UM-02\n",
      "3             3  Maximus UM-03,)\n",
      "(   sk_Segmento  NOME_SEGMENTO\n",
      "0            0  Maximus UM-00\n",
      "1            1  Maximus UM-01\n",
      "2            2  Maximus UM-02\n",
      "3            3  Maximus UM-03\n",
      "4            4  Maximus UM-04\n",
      "5            5  Maximus UM-05\n",
      "6            6  Maximus UM-06\n",
      "7            7  Maximus UM-07,)\n",
      "(Empty DataFrame\n",
      "Columns: [sk_Fabricante, NOME_FABRICANTE]\n",
      "Index: [],)\n",
      "(        sk_Nome          NOME_NOME\n",
      "0             0      Maximus UM-00\n",
      "1             1      Maximus UM-01\n",
      "2             2      Maximus UM-02\n",
      "3             3      Maximus UM-03\n",
      "4             4      Maximus UM-04\n",
      "...         ...                ...\n",
      "250816   250816  Maximus UM-250816\n",
      "250817   250817  Maximus UM-250817\n",
      "250818   250818  Maximus UM-250818\n",
      "250819   250819  Maximus UM-250819\n",
      "250820   250820  Maximus UM-250820\n",
      "\n",
      "[250821 rows x 2 columns],)\n",
      "(       sk_Cidade       NOME_CIDADE\n",
      "0              0     Maximus UM-00\n",
      "1              1     Maximus UM-01\n",
      "2              2     Maximus UM-02\n",
      "3              3     Maximus UM-03\n",
      "4              4     Maximus UM-04\n",
      "...          ...               ...\n",
      "14512      14512  Maximus UM-14512\n",
      "14513      14513  Maximus UM-14513\n",
      "14514      14514  Maximus UM-14514\n",
      "14515      14515  Maximus UM-14515\n",
      "14516      14516  Maximus UM-14516\n",
      "\n",
      "[14517 rows x 2 columns],)\n",
      "(    sk_Estado    NOME_ESTADO\n",
      "0           0  Maximus UM-00\n",
      "1           1  Maximus UM-01\n",
      "2           2  Maximus UM-02\n",
      "3           3  Maximus UM-03\n",
      "4           4  Maximus UM-04\n",
      "5           5  Maximus UM-05\n",
      "6           6  Maximus UM-06\n",
      "7           7  Maximus UM-07\n",
      "8           8  Maximus UM-08\n",
      "9           9  Maximus UM-09\n",
      "10         10  Maximus UM-10\n",
      "11         11  Maximus UM-11\n",
      "12         12  Maximus UM-12\n",
      "13         13  Maximus UM-13\n",
      "14         14  Maximus UM-14\n",
      "15         15  Maximus UM-15\n",
      "16         16  Maximus UM-16\n",
      "17         17  Maximus UM-17\n",
      "18         18  Maximus UM-18\n",
      "19         19  Maximus UM-19\n",
      "20         20  Maximus UM-20\n",
      "21         21  Maximus UM-21\n",
      "22         22  Maximus UM-22\n",
      "23         23  Maximus UM-23\n",
      "24         24  Maximus UM-24\n",
      "25         25  Maximus UM-25\n",
      "26         26  Maximus UM-26\n",
      "27         27  Maximus UM-27\n",
      "28         28  Maximus UM-28\n",
      "29         29  Maximus UM-29\n",
      "30         30  Maximus UM-30\n",
      "31         31  Maximus UM-31\n",
      "32         32  Maximus UM-32\n",
      "33         33  Maximus UM-33\n",
      "34         34  Maximus UM-34\n",
      "35         35  Maximus UM-35\n",
      "36         36  Maximus UM-36\n",
      "37         37  Maximus UM-37\n",
      "38         38  Maximus UM-38\n",
      "39         39  Maximus UM-39\n",
      "40         40  Maximus UM-40\n",
      "41         41  Maximus UM-41\n",
      "42         42  Maximus UM-42\n",
      "43         43  Maximus UM-43\n",
      "44         44  Maximus UM-44\n",
      "45         45  Maximus UM-45\n",
      "46         46  Maximus UM-46\n",
      "47         47  Maximus UM-47,)\n",
      "(   sk_Regiao    NOME_REGIAO\n",
      "0          0  Maximus UM-00\n",
      "1          1  Maximus UM-01,)\n",
      "(Empty DataFrame\n",
      "Columns: [sk_Pais, NOME_PAIS]\n",
      "Index: [],)\n"
     ]
    }
   ],
   "source": [
    "# Aplicar SCD1 nas dimensões\n",
    "\n",
    "\n",
    "# Definir a função aplicar_scd1\n",
    "def aplicar_scd1(df_atual, df_novo, chave_primaria, atributo):\n",
    "    # Atualizar os registros existentes no DataFrame atual com os novos valores do DataFrame novo\n",
    "\n",
    "    df_atual.loc[df_atual[chave_primaria].isin(df_novo[chave_primaria]), atributo] = df_novo[atributo]\n",
    "    # Retornar o DataFrame atualizado\n",
    "    return df_atual\n",
    "\n",
    "# Criar DataFrames com os valores atuais para as dimensões (Produto, Categoria, Segmento, Fabricante, Nome, Cidade, Estado, Região, País)\n",
    "df_d_Produto_atual = pd.DataFrame({\n",
    "    'sk_Produto': list(range(172)), \n",
    "    'NOME_PRODUTO': [f'Maximus UM-{i:02d}' for i in range(172)]})\n",
    "df_d_Categoria_atual = pd.DataFrame({\n",
    "    'sk_Categoria': list(range(4)), \n",
    "    'NOME_CATEGORIA': [f'Maximus UM-{i:02d}' for i in range(4)]\n",
    "})\n",
    "df_d_Segmento_atual = pd.DataFrame({\n",
    "    'sk_Segmento': list(range(8)), \n",
    "    'NOME_SEGMENTO': [f'Maximus UM-{i:02d}' for i in range(8)]\n",
    "})\n",
    "df_d_Fabricante_atual = pd.DataFrame({\n",
    "    'sk_Fabricante': list(range(0)), \n",
    "    'NOME_FABRICANTE': [f'Maximus UM-{i:02d}' for i in range(0)]\n",
    "})    \n",
    "df_d_Nome_atual = pd.DataFrame({\n",
    "    'sk_Nome': list(range(250821)), \n",
    "    'NOME_NOME': [f'Maximus UM-{i:02d}' for i in range(250821)]\n",
    "})\n",
    "df_d_Cidade_atual = pd.DataFrame({\n",
    "    'sk_Cidade': list(range(14517)), \n",
    "    'NOME_CIDADE': [f'Maximus UM-{i:02d}' for i in range(14517)]\n",
    "})\n",
    "df_d_Estado_atual = pd.DataFrame({\n",
    "    'sk_Estado': list(range(48)), \n",
    "    'NOME_ESTADO': [f'Maximus UM-{i:02d}' for i in range(48)]\n",
    "})\n",
    "df_d_Regiao_atual = pd.DataFrame({\n",
    "    'sk_Regiao': list(range(2)), \n",
    "    'NOME_REGIAO': [f'Maximus UM-{i:02d}' for i in range(2)]\n",
    "})\n",
    "df_d_Pais_atual = pd.DataFrame({\n",
    "    'sk_Pais': list(range(0)), \n",
    "    'NOME_PAIS': [f'Maximus UM-{i:02d}' for i in range(0)]\n",
    "})\n",
    "\n",
    "\n",
    "# Criar DataFrames com os valores atuais para outras dimensões (Produto, Categoria, Segmento, Fabricante, Nome, Cidade, Estado, Região, País)\n",
    "df_d_Produto_novo = pd.DataFrame({\n",
    "    'sk_Produto': list(range(172)), \n",
    "    'NOME_PRODUTO': [f'Maximus UM-{i:02d}' for i in range(172)]\n",
    "})\n",
    "df_d_Categoria_novo = pd.DataFrame({\n",
    "    'sk_Categoria': list(range(4)), \n",
    "    'NOME_CATEGORIA': [f'Maximus UM-{i:02d}' for i in range(4)]\n",
    "})\n",
    "df_d_Segmento_novo = pd.DataFrame({\n",
    "    'sk_Segmento': list(range(8)), \n",
    "    'NOME_SEGMENTO': [f'Maximus UM-{i:02d}' for i in range(8)]\n",
    "})\n",
    "df_d_Fabricante_novo = pd.DataFrame({\n",
    "    'sk_Fabricante': list(range(0)), \n",
    "    'NOME_FABRICANTE': [f'Maximus UM-{i:02d}' for i in range(0)]\n",
    "})\n",
    "df_d_Nome_novo = pd.DataFrame({\n",
    "    'sk_Nome': list(range(250821)), \n",
    "    'NOME_NOME': [f'Maximus UM-{i:02d}' for i in range(250821)]\n",
    "})\n",
    "df_d_Cidade_novo = pd.DataFrame({\n",
    "    'sk_Cidade': list(range(14517)), \n",
    "    'NOME_CIDADE': [f'Maximus UM-{i:02d}' for i in range(14517)]\n",
    "})\n",
    "df_d_Estado_novo = pd.DataFrame({\n",
    "    'sk_Estado': list(range(48)), \n",
    "    'NOME_ESTADO': [f'Maximus UM-{i:02d}' for i in range(48)]\n",
    "})\n",
    "df_d_Regiao_novo = pd.DataFrame({\n",
    "    'sk_Regiao': list(range(2)), \n",
    "    'NOME_REGIAO': [f'Maximus UM-{i:02d}' for i in range(2)]\n",
    "})\n",
    "df_d_Pais_novo = pd.DataFrame({\n",
    "    'sk_Pais': list(range(0)), \n",
    "    'NOME_PAIS': [f'Maximus UM-{i:02d}' for i in range(0)]\n",
    "})\n",
    "\n",
    "# Remover os produtos que já existem no DataFrame atual\n",
    "df_d_Produto_novo = df_d_Produto_novo[df_d_Produto_novo['sk_Produto'].isin(df_d_Produto_atual['sk_Produto'])]\n",
    "df_d_Categoria_novo = df_d_Categoria_novo[df_d_Categoria_novo['sk_Categoria'].isin(df_d_Categoria_atual['sk_Categoria'])]\n",
    "df_d_Segmento_novo = df_d_Segmento_novo[df_d_Segmento_novo['sk_Segmento'].isin(df_d_Segmento_atual['sk_Segmento'])]\n",
    "df_d_Fabricante_novo = df_d_Fabricante_novo[df_d_Fabricante_novo['sk_Fabricante'].isin(df_d_Fabricante_atual['sk_Fabricante'])]\n",
    "df_d_Nome_novo = df_d_Nome_novo[df_d_Nome_novo['sk_Nome'].isin(df_d_Nome_atual['sk_Nome'])]\n",
    "df_d_Cidade_novo = df_d_Cidade_novo[df_d_Cidade_novo['sk_Cidade'].isin(df_d_Cidade_atual['sk_Cidade'])]\n",
    "df_d_Estado_novo = df_d_Estado_novo[df_d_Estado_novo['sk_Estado'].isin(df_d_Estado_atual['sk_Estado'])]\n",
    "df_d_Regiao_novo = df_d_Regiao_novo[df_d_Regiao_novo['sk_Regiao'].isin(df_d_Regiao_atual['sk_Regiao'])]\n",
    "df_d_Pais_novo = df_d_Pais_novo[df_d_Pais_novo['sk_Pais'].isin(df_d_Pais_atual['sk_Pais'])]\n",
    "\n",
    "\n",
    "# Aplicar SCD1 na dimensão de Atual\n",
    "df_d_Produto_atual = aplicar_scd1(df_d_Produto_atual, df_d_Produto_novo, 'sk_Produto', 'NOME_PRODUTO'),\n",
    "df_d_Categoria_atual = aplicar_scd1(df_d_Categoria_atual, df_d_Categoria_novo, 'sk_Categoria', 'NOME_CATEGORIA'),\n",
    "df_d_Segmento_atual = aplicar_scd1(df_d_Segmento_atual, df_d_Segmento_novo, 'sk_Segmento', 'NOME_SEGMENTO'),\n",
    "df_d_Fabricante_atual = aplicar_scd1(df_d_Fabricante_novo, df_d_Fabricante_novo, 'sk_Fabricante', 'NOME_FABRICANTE'),\n",
    "df_d_Nome_atual = aplicar_scd1(df_d_Nome_novo, df_d_Nome_novo, 'sk_Nome', 'NOME_NOME'),\n",
    "df_d_Cidade_atual = aplicar_scd1(df_d_Cidade_novo, df_d_Cidade_novo, 'sk_Cidade', 'NOME_CIDADE'),\n",
    "df_d_Estado_atual = aplicar_scd1(df_d_Estado_novo, df_d_Estado_novo, 'sk_Estado', 'NOME_ESTADO'),\n",
    "df_d_Regiao_atual = aplicar_scd1(df_d_Regiao_novo, df_d_Regiao_novo, 'sk_Regiao', 'NOME_REGIAO'),\n",
    "df_d_Pais_atual = aplicar_scd1(df_d_Pais_novo, df_d_Pais_novo, 'sk_Pais', 'NOME_PAIS'),\n",
    "\n",
    "\n",
    "print(\"\\nDataFrame atualizado após aplicar SCD1:\")\n",
    "print(df_d_Produto_atual)\n",
    "print(df_d_Categoria_atual)\n",
    "print(df_d_Segmento_atual)\n",
    "print(df_d_Fabricante_atual)\n",
    "print(df_d_Nome_atual)\n",
    "print(df_d_Cidade_atual)\n",
    "print(df_d_Estado_atual)\n",
    "print(df_d_Regiao_atual)\n",
    "print(df_d_Pais_atual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDProduto</th>\n",
       "      <th>Data</th>\n",
       "      <th>IDCliente</th>\n",
       "      <th>Unidades</th>\n",
       "      <th>Produto</th>\n",
       "      <th>Categoria</th>\n",
       "      <th>Segmento</th>\n",
       "      <th>IDFabricante</th>\n",
       "      <th>Fabricante</th>\n",
       "      <th>CustoUnitario</th>\n",
       "      <th>PrecoUnitario</th>\n",
       "      <th>EmailNome</th>\n",
       "      <th>Cidade</th>\n",
       "      <th>Estado</th>\n",
       "      <th>Regiao</th>\n",
       "      <th>Pais</th>\n",
       "      <th>Email</th>\n",
       "      <th>Nome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>449</td>\n",
       "      <td>2012-07-26 00:00:00.000</td>\n",
       "      <td>247546</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-54</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Moderation</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>74.729917</td>\n",
       "      <td>102.36975</td>\n",
       "      <td>(Nerea.Barry@xyza.com): Barry, Nerea</td>\n",
       "      <td>Miami</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Nerea.Barry@xyza.com)</td>\n",
       "      <td>Barry Nerea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>449</td>\n",
       "      <td>2013-10-31 00:00:00.000</td>\n",
       "      <td>124593</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-54</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Moderation</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>74.729917</td>\n",
       "      <td>102.36975</td>\n",
       "      <td>(Elliott.Stuart@xyza.com): Stuart, Elliott</td>\n",
       "      <td>Miami</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Elliott.Stuart@xyza.com)</td>\n",
       "      <td>Stuart Elliott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>449</td>\n",
       "      <td>2013-11-14 00:00:00.000</td>\n",
       "      <td>163517</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-54</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Moderation</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>74.729917</td>\n",
       "      <td>102.36975</td>\n",
       "      <td>(Holmes.Swanson@xyza.com): Swanson, Holmes</td>\n",
       "      <td>Miami</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Holmes.Swanson@xyza.com)</td>\n",
       "      <td>Swanson Holmes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>449</td>\n",
       "      <td>2013-01-17 00:00:00.000</td>\n",
       "      <td>8875</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-54</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Moderation</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>74.729917</td>\n",
       "      <td>102.36975</td>\n",
       "      <td>(Nyssa.Solis@xyza.com): Solis, Nyssa</td>\n",
       "      <td>Miami</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Nyssa.Solis@xyza.com)</td>\n",
       "      <td>Solis Nyssa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>449</td>\n",
       "      <td>2014-09-13 00:00:00.000</td>\n",
       "      <td>8894</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-54</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Moderation</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>74.729917</td>\n",
       "      <td>102.36975</td>\n",
       "      <td>(Ivana.Wall@xyza.com): Wall, Ivana</td>\n",
       "      <td>Miami</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Ivana.Wall@xyza.com)</td>\n",
       "      <td>Wall Ivana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675363</th>\n",
       "      <td>407</td>\n",
       "      <td>2012-05-20 00:00:00.000</td>\n",
       "      <td>32464</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-12</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>116.887417</td>\n",
       "      <td>160.11975</td>\n",
       "      <td>(Catherine.Martinez@xyza.com): Martinez, Cathe...</td>\n",
       "      <td>Panama City</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Catherine.Martinez@xyza.com)</td>\n",
       "      <td>Martinez Catherine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675364</th>\n",
       "      <td>407</td>\n",
       "      <td>2011-03-02 00:00:00.000</td>\n",
       "      <td>32441</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-12</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>116.887417</td>\n",
       "      <td>160.11975</td>\n",
       "      <td>(Dora.Emerson@xyza.com): Emerson, Dora</td>\n",
       "      <td>Panama City</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Dora.Emerson@xyza.com)</td>\n",
       "      <td>Emerson Dora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675365</th>\n",
       "      <td>407</td>\n",
       "      <td>2012-06-02 00:00:00.000</td>\n",
       "      <td>101533</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-12</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>116.887417</td>\n",
       "      <td>160.11975</td>\n",
       "      <td>(Lynn.Jones@xyza.com): Jones, Lynn</td>\n",
       "      <td>Pensacola</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Lynn.Jones@xyza.com)</td>\n",
       "      <td>Jones Lynn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675366</th>\n",
       "      <td>407</td>\n",
       "      <td>2011-04-10 00:00:00.000</td>\n",
       "      <td>147192</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-12</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>116.887417</td>\n",
       "      <td>160.11975</td>\n",
       "      <td>(Maite.Cantrell@xyza.com): Cantrell, Maite</td>\n",
       "      <td>Panama City Beach</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Maite.Cantrell@xyza.com)</td>\n",
       "      <td>Cantrell Maite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675367</th>\n",
       "      <td>407</td>\n",
       "      <td>2012-03-01 00:00:00.000</td>\n",
       "      <td>237766</td>\n",
       "      <td>1</td>\n",
       "      <td>Maximus UM-12</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>Accessory</td>\n",
       "      <td>7</td>\n",
       "      <td>VanArsdel</td>\n",
       "      <td>116.887417</td>\n",
       "      <td>160.11975</td>\n",
       "      <td>(Yolanda.Byers@xyza.com): Byers, Yolanda</td>\n",
       "      <td>Fountain</td>\n",
       "      <td>FL</td>\n",
       "      <td>East</td>\n",
       "      <td>USA</td>\n",
       "      <td>(Yolanda.Byers@xyza.com)</td>\n",
       "      <td>Byers Yolanda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>675368 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        IDProduto                     Data  IDCliente  Unidades  \\\n",
       "0             449  2012-07-26 00:00:00.000     247546         1   \n",
       "1             449  2013-10-31 00:00:00.000     124593         1   \n",
       "2             449  2013-11-14 00:00:00.000     163517         1   \n",
       "3             449  2013-01-17 00:00:00.000       8875         1   \n",
       "4             449  2014-09-13 00:00:00.000       8894         1   \n",
       "...           ...                      ...        ...       ...   \n",
       "675363        407  2012-05-20 00:00:00.000      32464         1   \n",
       "675364        407  2011-03-02 00:00:00.000      32441         1   \n",
       "675365        407  2012-06-02 00:00:00.000     101533         1   \n",
       "675366        407  2011-04-10 00:00:00.000     147192         1   \n",
       "675367        407  2012-03-01 00:00:00.000     237766         1   \n",
       "\n",
       "              Produto  Categoria    Segmento  IDFabricante Fabricante  \\\n",
       "0       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
       "1       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
       "2       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
       "3       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
       "4       Maximus UM-54      Urban  Moderation             7  VanArsdel   \n",
       "...               ...        ...         ...           ...        ...   \n",
       "675363  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
       "675364  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
       "675365  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
       "675366  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
       "675367  Maximus UM-12  Accessory   Accessory             7  VanArsdel   \n",
       "\n",
       "        CustoUnitario  PrecoUnitario  \\\n",
       "0           74.729917      102.36975   \n",
       "1           74.729917      102.36975   \n",
       "2           74.729917      102.36975   \n",
       "3           74.729917      102.36975   \n",
       "4           74.729917      102.36975   \n",
       "...               ...            ...   \n",
       "675363     116.887417      160.11975   \n",
       "675364     116.887417      160.11975   \n",
       "675365     116.887417      160.11975   \n",
       "675366     116.887417      160.11975   \n",
       "675367     116.887417      160.11975   \n",
       "\n",
       "                                                EmailNome             Cidade  \\\n",
       "0                    (Nerea.Barry@xyza.com): Barry, Nerea              Miami   \n",
       "1              (Elliott.Stuart@xyza.com): Stuart, Elliott              Miami   \n",
       "2              (Holmes.Swanson@xyza.com): Swanson, Holmes              Miami   \n",
       "3                    (Nyssa.Solis@xyza.com): Solis, Nyssa              Miami   \n",
       "4                      (Ivana.Wall@xyza.com): Wall, Ivana              Miami   \n",
       "...                                                   ...                ...   \n",
       "675363  (Catherine.Martinez@xyza.com): Martinez, Cathe...        Panama City   \n",
       "675364             (Dora.Emerson@xyza.com): Emerson, Dora        Panama City   \n",
       "675365                 (Lynn.Jones@xyza.com): Jones, Lynn          Pensacola   \n",
       "675366         (Maite.Cantrell@xyza.com): Cantrell, Maite  Panama City Beach   \n",
       "675367           (Yolanda.Byers@xyza.com): Byers, Yolanda           Fountain   \n",
       "\n",
       "       Estado Regiao Pais                          Email                Nome  \n",
       "0          FL   East  USA         (Nerea.Barry@xyza.com)         Barry Nerea  \n",
       "1          FL   East  USA      (Elliott.Stuart@xyza.com)      Stuart Elliott  \n",
       "2          FL   East  USA      (Holmes.Swanson@xyza.com)      Swanson Holmes  \n",
       "3          FL   East  USA         (Nyssa.Solis@xyza.com)         Solis Nyssa  \n",
       "4          FL   East  USA          (Ivana.Wall@xyza.com)          Wall Ivana  \n",
       "...       ...    ...  ...                            ...                 ...  \n",
       "675363     FL   East  USA  (Catherine.Martinez@xyza.com)  Martinez Catherine  \n",
       "675364     FL   East  USA        (Dora.Emerson@xyza.com)        Emerson Dora  \n",
       "675365     FL   East  USA          (Lynn.Jones@xyza.com)          Jones Lynn  \n",
       "675366     FL   East  USA      (Maite.Cantrell@xyza.com)      Cantrell Maite  \n",
       "675367     FL   East  USA       (Yolanda.Byers@xyza.com)       Byers Yolanda  \n",
       "\n",
       "[675368 rows x 18 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original\n",
    "\n",
    "# Só para validar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sk_Produto  sk_Categoria  sk_Segmento  sk_Fabricante  sk_Nome  \\\n",
      "0                0             0            0              0        1   \n",
      "1                0             0            0              0        2   \n",
      "2                0             0            0              0        3   \n",
      "3                0             0            0              0        4   \n",
      "4                0             0            0              0        5   \n",
      "...            ...           ...          ...            ...      ...   \n",
      "675363          23             4            8              0     3998   \n",
      "675364          23             4            8              0     3881   \n",
      "675365          23             4            8              0   239100   \n",
      "675366          23             4            8              0   178370   \n",
      "675367          23             4            8              0   141054   \n",
      "\n",
      "        sk_Cidade  sk_Estado  sk_Regiao  sk_Pais                     Data  \\\n",
      "0               0          0          0        0  2012-07-26 00:00:00.000   \n",
      "1               0          0          0        0  2013-10-31 00:00:00.000   \n",
      "2               0          0          0        0  2013-11-14 00:00:00.000   \n",
      "3               0          0          0        0  2013-01-17 00:00:00.000   \n",
      "4               0          0          0        0  2014-09-13 00:00:00.000   \n",
      "...           ...        ...        ...      ...                      ...   \n",
      "675363        289          0          0        0  2012-05-20 00:00:00.000   \n",
      "675364        289          0          0        0  2011-03-02 00:00:00.000   \n",
      "675365        287          0          0        0  2012-06-02 00:00:00.000   \n",
      "675366        294          0          0        0  2011-04-10 00:00:00.000   \n",
      "675367       2718          0          0        0  2012-03-01 00:00:00.000   \n",
      "\n",
      "        Unidades  CustoUnitario  PrecoUnitario  \n",
      "0              1      74.729917      102.36975  \n",
      "1              1      74.729917      102.36975  \n",
      "2              1      74.729917      102.36975  \n",
      "3              1      74.729917      102.36975  \n",
      "4              1      74.729917      102.36975  \n",
      "...          ...            ...            ...  \n",
      "675363         1     116.887417      160.11975  \n",
      "675364         1     116.887417      160.11975  \n",
      "675365         1     116.887417      160.11975  \n",
      "675366         1     116.887417      160.11975  \n",
      "675367         1     116.887417      160.11975  \n",
      "\n",
      "[675368 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# JUNÇÃO DOS DATA FRAMES\n",
    "\n",
    "result = df_original.merge(df_d_Produto, left_on='Produto', right_on='NOME_PRODUTO', how='left')\n",
    "result = result.merge(df_d_Categoria, left_on='Categoria', right_on='NOME_CATEGORIA', how='left')\n",
    "result = result.merge(df_d_Segmento, left_on='Segmento', right_on='NOME_SEGMENTO', how='left')\n",
    "result = result.merge(df_d_Fabricante, left_on='Fabricante', right_on='NOME_FABRICANTE', how='left')\n",
    "result = result.merge(df_d_Nome, left_on='Nome', right_on='NOME_NOME', how='left')\n",
    "result = result.merge(df_d_Cidade, left_on='Cidade', right_on='NOME_CIDADE', how='left')\n",
    "result = result.merge(df_d_Estado, left_on='Estado', right_on='NOME_ESTADO', how='left')\n",
    "result = result.merge(df_d_Regiao, left_on='Regiao', right_on='NOME_REGIAO', how='left')\n",
    "result = result.merge(df_d_Pais, left_on='Pais', right_on='NOME_PAIS', how='left')\n",
    "\n",
    "# SUBSTITUIÇAO DE VALORES NULOS\n",
    "\n",
    "result['sk_Produto'] = result['sk_Produto'].fillna('N/A')\n",
    "result['sk_Categoria'] = result['sk_Categoria'].fillna('N/A')\n",
    "result['sk_Segmento'] = result['sk_Segmento'].fillna('N/A')\n",
    "result['sk_Fabricante'] = result['sk_Fabricante'].fillna('N/A')\n",
    "result['sk_Nome'] = result['sk_Nome'].fillna('N/A')\n",
    "result['sk_Cidade'] = result['sk_Cidade'].fillna('N/A')\n",
    "result['sk_Estado'] = result['sk_Estado'].fillna('N/A')\n",
    "result['sk_Regiao'] = result['sk_Regiao'].fillna('N/A')\n",
    "result['sk_Pais'] = result['sk_Pais'].fillna('N/A')\n",
    "result.head()\n",
    "df_fato_venda = result[['sk_Produto', 'sk_Categoria', 'sk_Segmento', 'sk_Fabricante', 'sk_Nome', 'sk_Cidade', 'sk_Estado', 'sk_Regiao', 'sk_Pais', 'Data', 'Unidades', 'CustoUnitario', 'PrecoUnitario']]\n",
    "\n",
    "print(df_fato_venda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envio de arquivos Dimensão para pasta GOLD em csv\n",
    "\n",
    "df_fato_venda.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\fato_vendas.csv', index=False)\n",
    "df_d_Produto.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\d_Produto.csv', index=False)\n",
    "df_d_Categoria.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\d_Categoria', index=False)\n",
    "df_d_Segmento.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\d_Segmento', index=False)\n",
    "df_d_Fabricante.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\d_Fabricante', index=False)\n",
    "df_d_Nome.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\d_Nome', index=False)\n",
    "df_d_Cidade.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\d_Cidade', index=False)\n",
    "df_d_Estado.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\d_Estado', index=False)\n",
    "df_d_Regiao.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\d_Regiao', index=False)\n",
    "df_d_Pais.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\csv\\d_Pais', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envio de arquivos Parquet para camada GOLD em pasta Arquivos Parquet\n",
    "\n",
    "df_fato_venda.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\fato_vendas.parquet', index=False)\n",
    "df_d_Produto.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Produto.parquet', index=False)\n",
    "df_d_Categoria.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Categoria.parquet', index=False)\n",
    "df_d_Segmento.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Segmento.parquet', index=False)\n",
    "df_d_Fabricante.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Fabricante.parquet', index=False)\n",
    "df_d_Nome.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Nome.parquet', index=False)\n",
    "df_d_Cidade.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Cidade.parquet', index=False)\n",
    "df_d_Estado.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Estado.parquet', index=False)\n",
    "df_d_Regiao.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Regiao.parquet', index=False)\n",
    "df_d_Pais.to_parquet(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Pais.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\fato_vendas.parquet\n",
      "   sk_Produto  sk_Categoria  sk_Segmento  sk_Fabricante  sk_Nome  sk_Cidade  \\\n",
      "0           0             0            0              0        1          0   \n",
      "1           0             0            0              0        2          0   \n",
      "2           0             0            0              0        3          0   \n",
      "3           0             0            0              0        4          0   \n",
      "4           0             0            0              0        5          0   \n",
      "\n",
      "   sk_Estado  sk_Regiao  sk_Pais                     Data  Unidades  \\\n",
      "0          0          0        0  2012-07-26 00:00:00.000         1   \n",
      "1          0          0        0  2013-10-31 00:00:00.000         1   \n",
      "2          0          0        0  2013-11-14 00:00:00.000         1   \n",
      "3          0          0        0  2013-01-17 00:00:00.000         1   \n",
      "4          0          0        0  2014-09-13 00:00:00.000         1   \n",
      "\n",
      "   CustoUnitario  PrecoUnitario  \n",
      "0      74.729917      102.36975  \n",
      "1      74.729917      102.36975  \n",
      "2      74.729917      102.36975  \n",
      "3      74.729917      102.36975  \n",
      "4      74.729917      102.36975  \n",
      "\n",
      "\n",
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Produto.parquet\n",
      "   sk_Produto   NOME_PRODUTO\n",
      "0           0  Maximus UM-54\n",
      "1           1  Maximus UM-75\n",
      "2           2  Maximus UM-01\n",
      "3           3  Maximus UM-62\n",
      "4           4  Maximus UM-38\n",
      "\n",
      "\n",
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Categoria.parquet\n",
      "   sk_Categoria NOME_CATEGORIA\n",
      "0             0          Urban\n",
      "1             1            Mix\n",
      "2             2          Youth\n",
      "3             3          Rural\n",
      "4             4      Accessory\n",
      "\n",
      "\n",
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Segmento.parquet\n",
      "   sk_Segmento NOME_SEGMENTO\n",
      "0            0    Moderation\n",
      "1            1   Convenience\n",
      "2            2       Extreme\n",
      "3            3       Regular\n",
      "4            4    All Season\n",
      "\n",
      "\n",
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Fabricante.parquet\n",
      "   sk_Fabricante NOME_FABRICANTE\n",
      "0              0       VanArsdel\n",
      "\n",
      "\n",
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Nome.parquet\n",
      "   sk_Nome       NOME_NOME\n",
      "0        1     Barry Nerea\n",
      "1        2  Stuart Elliott\n",
      "2        3  Swanson Holmes\n",
      "3        4     Solis Nyssa\n",
      "4        5      Wall Ivana\n",
      "\n",
      "\n",
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Cidade.parquet\n",
      "   sk_Cidade       NOME_CIDADE\n",
      "0          0             Miami\n",
      "1          1   Fort Lauderdale\n",
      "2          2  Port Saint Lucie\n",
      "3          3        Fort Myers\n",
      "4          4             Tampa\n",
      "\n",
      "\n",
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Estado.parquet\n",
      "   sk_Estado NOME_ESTADO\n",
      "0          0          FL\n",
      "1          1          OH\n",
      "2          2          PA\n",
      "3          3          NY\n",
      "4          4          MI\n",
      "\n",
      "\n",
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Regiao.parquet\n",
      "   sk_Regiao NOME_REGIAO\n",
      "0          0        East\n",
      "1          1     Central\n",
      "2          2        West\n",
      "\n",
      "\n",
      "Dados de C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Pais.parquet\n",
      "   sk_Pais NOME_PAIS\n",
      "0        0       USA\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Caminho dos arquivos para ler arquivos\n",
    "\n",
    "arquivos = [\n",
    "\n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\fato_vendas.parquet', \n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Produto.parquet', \n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Categoria.parquet', \n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Segmento.parquet', \n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Fabricante.parquet', \n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Nome.parquet',\n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Cidade.parquet', \n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Estado.parquet', \n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Regiao.parquet',\n",
    "r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\Arquivos Parquet\\d_Pais.parquet',\n",
    "\n",
    "]\n",
    "\n",
    "# Ler e exibir os primeiros registros de cada arquivo\n",
    "\n",
    "for arquivo in arquivos:\n",
    "    df = pd.read_parquet(arquivo)\n",
    "    print(f\"Dados de {arquivo}\")\n",
    "    print(df.head()) # Exibe os primeiros 5 registros por padrão \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As colunas 'Ano' e 'Mes' foram criadas na tabela original.\n",
      "   IDProduto       Data  IDCliente  Unidades        Produto Categoria  \\\n",
      "0        449 2012-07-26     247546         1  Maximus UM-54     Urban   \n",
      "1        449 2013-10-31     124593         1  Maximus UM-54     Urban   \n",
      "2        449 2013-11-14     163517         1  Maximus UM-54     Urban   \n",
      "3        449 2013-01-17       8875         1  Maximus UM-54     Urban   \n",
      "4        449 2014-09-13       8894         1  Maximus UM-54     Urban   \n",
      "\n",
      "     Segmento  IDFabricante Fabricante  CustoUnitario  PrecoUnitario  \\\n",
      "0  Moderation             7  VanArsdel      74.729917      102.36975   \n",
      "1  Moderation             7  VanArsdel      74.729917      102.36975   \n",
      "2  Moderation             7  VanArsdel      74.729917      102.36975   \n",
      "3  Moderation             7  VanArsdel      74.729917      102.36975   \n",
      "4  Moderation             7  VanArsdel      74.729917      102.36975   \n",
      "\n",
      "                                    EmailNome Cidade Estado Regiao Pais  \\\n",
      "0        (Nerea.Barry@xyza.com): Barry, Nerea  Miami     FL   East  USA   \n",
      "1  (Elliott.Stuart@xyza.com): Stuart, Elliott  Miami     FL   East  USA   \n",
      "2  (Holmes.Swanson@xyza.com): Swanson, Holmes  Miami     FL   East  USA   \n",
      "3        (Nyssa.Solis@xyza.com): Solis, Nyssa  Miami     FL   East  USA   \n",
      "4          (Ivana.Wall@xyza.com): Wall, Ivana  Miami     FL   East  USA   \n",
      "\n",
      "                       Email            Nome   Ano  Mes  \n",
      "0     (Nerea.Barry@xyza.com)     Barry Nerea  2012    7  \n",
      "1  (Elliott.Stuart@xyza.com)  Stuart Elliott  2013   10  \n",
      "2  (Holmes.Swanson@xyza.com)  Swanson Holmes  2013   11  \n",
      "3     (Nyssa.Solis@xyza.com)     Solis Nyssa  2013    1  \n",
      "4      (Ivana.Wall@xyza.com)      Wall Ivana  2014    9  \n"
     ]
    }
   ],
   "source": [
    "# Particionar os arquivos Parquet da tabela fato em ano e mês\n",
    "\n",
    "# Carregar o arquivo em um DataFrame diretamente\n",
    "df_fato_venda = pd.read_csv(r\"C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv\")\n",
    "\n",
    "# Convertendo a coluna 'Data' para o formato datetime\n",
    "df_fato_venda['Data'] = pd.to_datetime(df_fato_venda['Data'])\n",
    "\n",
    "# Criando colunas separadas para ano e mês\n",
    "df_fato_venda['Ano'] = df_fato_venda['Data'].dt.year\n",
    "df_fato_venda['Mes'] = df_fato_venda['Data'].dt.month\n",
    "\n",
    "# Salvar a converçao da coluna 'Data' para ano e mês no mesmo arquivo CSV  \n",
    "df_original.to_csv(r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\BRONZE\\Vendas.csv', index=False) \n",
    "\n",
    "# Particionando os dados por ano e mês ao salvar em parquet\n",
    "df_fato_venda.to_parquet(\n",
    "    path=r'C:\\Users\\ferna\\OneDrive\\Área de Trabalho\\DESAFIO_DWE19.ipynb\\GOLD\\fato_vendas_particionado',\n",
    "    partition_cols=['Ano', 'Mes'],\n",
    "    index=False\n",
    "    )\n",
    "\n",
    "# Verificar se as colunas 'Ano' e 'Mes' já existem no DataFrame\n",
    "columns_already_created = all(col in df_fato_venda.columns for col in ['Ano', 'Mes'])\n",
    "\n",
    "# Se não\n",
    "if not columns_already_created:\n",
    "    print(\"Executar código\")\n",
    "else:\n",
    "    print(\"As colunas 'Ano' e 'Mes' foram criadas na tabela original.\")\n",
    "    print(df_fato_venda.head())  # Exibir as primeiras linhas do DataFrame df_fato_venda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
